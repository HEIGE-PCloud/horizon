{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(547, 2) (11832, 2)\n",
      "0     In Trading is a vertically integrated Solid W...\n",
      "1    We believe cities are complicated. And your mo...\n",
      "2    Smarter GoodÂ is an innovative global services ...\n",
      "3    Join the Your Phone Connect team and earn mone...\n",
      "4    At Total EclipseÂ we create fun, original, and ...\n",
      "Name: job_description, dtype: object\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    1\n",
      "4    0\n",
      "Name: fraudulent, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from util import load_data, split_data\n",
    "data = load_data(Path(\"data/fake_job_postings.csv\"))\n",
    "X_train, X_test, y_train, y_test = split_data(data)\n",
    "print(X_train.head())\n",
    "print(y_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/jchen/Code/job_catcher/.venv/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='4374' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  51/4374 00:28 < 41:41, 1.73 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Custom Dataset to handle our job descriptions and labels\n",
    "class JobDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (pd.Series or list): The job descriptions.\n",
    "            labels (pd.Series or list): The corresponding labels (0 or 1).\n",
    "            tokenizer (PreTrainedTokenizer): Tokenizer for BERT.\n",
    "            max_length (int): Maximum token length for each text.\n",
    "        \"\"\"\n",
    "        # Convert pandas Series to list if needed\n",
    "        self.texts = texts.tolist() if hasattr(texts, 'tolist') else texts\n",
    "        self.labels = labels.tolist() if hasattr(labels, 'tolist') else labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        # Tokenize the text with truncation and padding\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        # Squeeze to remove the batch dimension\n",
    "        encoding = {key: tensor.squeeze(0) for key, tensor in encoding.items()}\n",
    "        encoding['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "        return encoding\n",
    "\n",
    "# Load the pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test have been defined as in your preprocessing step\n",
    "# Create our dataset objects\n",
    "train_dataset = JobDataset(X_train, y_train, tokenizer, max_length=128)\n",
    "test_dataset = JobDataset(X_test, y_test, tokenizer, max_length=128)\n",
    "\n",
    "# Load pre-trained BERT model for sequence classification (binary classification => num_labels=2)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Define a metric function for evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',             # output directory\n",
    "    num_train_epochs=3,                 # total number of training epochs\n",
    "    per_device_train_batch_size=16,     # batch size per device during training\n",
    "    per_device_eval_batch_size=16,      # batch size for evaluation\n",
    "    evaluation_strategy='epoch',        # evaluate at the end of each epoch\n",
    "    save_strategy='epoch',              # save checkpoint at the end of each epoch\n",
    "    logging_dir='./logs',               # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,        # load the best model when finished training\n",
    "    metric_for_best_model=\"accuracy\",   # use accuracy to evaluate the best model\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Optionally, evaluate the model on the test set after training\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
