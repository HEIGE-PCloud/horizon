{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6wVvWIqVo6e"
      },
      "source": [
        "## Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nrxrPN_1VvdQ"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def load_data(path: Path) -> pd.DataFrame:\n",
        "    data = pd.read_csv(path)\n",
        "    data = pd.DataFrame(\n",
        "        {\n",
        "            \"job_description\": data[\n",
        "                [\"company_profile\", \"description\", \"requirements\", \"benefits\"]\n",
        "            ]\n",
        "            .fillna(\"\")\n",
        "            .agg(\" \".join, axis=1),\n",
        "            \"fraudulent\": data[\"fraudulent\"],\n",
        "        }\n",
        "    )\n",
        "    data = data.drop_duplicates(subset=[\"job_description\"], keep=\"first\")\n",
        "    return data\n",
        "\n",
        "def clean_text(text):\n",
        "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
        "    and remove words containing numbers.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def split_data(data: pd.DataFrame):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        data[\"job_description\"],\n",
        "        data[\"fraudulent\"],\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=data[\"fraudulent\"],\n",
        "    )\n",
        "\n",
        "    # Convert to DataFrame for easy manipulation\n",
        "    train_df = pd.DataFrame({'job_description': X_train, 'fraudulent': y_train})\n",
        "\n",
        "    # Separate fraudulent (y=1) and non-fraudulent (y=0) samples\n",
        "    fraudulent_df = train_df[train_df['fraudulent'] == 1]\n",
        "    non_fraudulent_df = train_df[train_df['fraudulent'] == 0]\n",
        "    print(fraudulent_df.shape, non_fraudulent_df.shape)\n",
        "\n",
        "    # Oversample each fraudulent job description exactly 21 times\n",
        "    fraudulent_df_oversampled = pd.concat([fraudulent_df] * 21, ignore_index=True)\n",
        "\n",
        "    # Combine the oversampled fraudulent data with the original non-fraudulent data\n",
        "    train_df_oversampled = pd.concat([non_fraudulent_df, fraudulent_df_oversampled], ignore_index=True)\n",
        "\n",
        "    # Shuffle the data\n",
        "    train_df_oversampled = train_df_oversampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # Extract the oversampled X_train and y_train\n",
        "    X_train: pd.Series[str] = train_df_oversampled['job_description']\n",
        "    y_train: pd.Series[int] = train_df_oversampled['fraudulent']\n",
        "\n",
        "    # apply clean_text function to the training and test data\n",
        "\n",
        "    X_train = X_train.apply(clean_text)\n",
        "    X_test = X_test.apply(clean_text)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgy-7bV3n2u-"
      },
      "source": [
        "## Download the Dataset from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6QZyqxRVo6k",
        "outputId": "7e6f9f2d-05f2-439a-9ab1-a02edb504493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.7), please consider upgrading to the latest version (0.3.8).\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/shivamb/real-or-fake-fake-jobposting-prediction/versions/1\n",
            "(547, 2) (11832, 2)\n",
            "0     in trading is a vertically integrated solid w...\n",
            "1    we believe cities are complicated and your mob...\n",
            "2    smarter good is an innovative global services ...\n",
            "3    join the your phone connect team and earn mone...\n",
            "4    at total eclipse we create fun original and ad...\n",
            "Name: job_description, dtype: object\n",
            "0    0\n",
            "1    0\n",
            "2    0\n",
            "3    1\n",
            "4    0\n",
            "Name: fraudulent, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "from pathlib import Path\n",
        "\n",
        "path = kagglehub.dataset_download(\"shivamb/real-or-fake-fake-jobposting-prediction\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "data = load_data(Path(path + \"/fake_job_postings.csv\"))\n",
        "\n",
        "X_train, X_test, y_train, y_test = split_data(data)\n",
        "\n",
        "print(X_train.head())\n",
        "print(y_train.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEYsTCEEn2u-"
      },
      "source": [
        "## Define the Dataset loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BWdoh5vOn2u-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class JobDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            texts (pd.Series or list): The job descriptions.\n",
        "            labels (pd.Series or list): The corresponding labels (0 or 1).\n",
        "            tokenizer (PreTrainedTokenizer): Tokenizer for BERT.\n",
        "            max_length (int): Maximum token length for each text.\n",
        "        \"\"\"\n",
        "        # Convert pandas Series to list if needed\n",
        "        self.texts = texts.tolist() if hasattr(texts, 'tolist') else texts\n",
        "        self.labels = labels.tolist() if hasattr(labels, 'tolist') else labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "        # Tokenize the text with truncation and padding\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        # Squeeze to remove the batch dimension\n",
        "        encoding = {key: tensor.squeeze(0) for key, tensor in encoding.items()}\n",
        "        encoding['labels'] = torch.tensor(label, dtype=torch.long)\n",
        "        return encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pCM2nq2fn2u_"
      },
      "outputs": [],
      "source": [
        "base_model_name = \"bert-large-uncased\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gEzQZZqQn2u_"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "train_dataset = JobDataset(X_train, y_train, tokenizer, max_length=512)\n",
        "test_dataset = JobDataset(X_test, y_test, tokenizer, max_length=512)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdg7qLw-n2u_",
        "outputId": "76aa4eb7-f055-4146-b12f-5cdd242744fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "classification_model = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "k0yrSINjVo6l",
        "outputId": "5655b5ce-4d0d-4165-bdb6-77155a1cce06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpclouduwu\u001b[0m (\u001b[33mpclouduwu-imperial-college-london\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250219_172345-f5doskb2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/pclouduwu-imperial-college-london/huggingface/runs/f5doskb2' target=\"_blank\">bert</a></strong> to <a href='https://wandb.ai/pclouduwu-imperial-college-london/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/pclouduwu-imperial-college-london/huggingface' target=\"_blank\">https://wandb.ai/pclouduwu-imperial-college-london/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/pclouduwu-imperial-college-london/huggingface/runs/f5doskb2' target=\"_blank\">https://wandb.ai/pclouduwu-imperial-college-london/huggingface/runs/f5doskb2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 24.12 MiB is free. Process 4520 has 14.71 GiB memory in use. Of the allocated memory 14.50 GiB is allocated by PyTorch, and 96.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-02fc2e52eda7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Fine-tune the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2171\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2172\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2173\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2529\u001b[0m                     )\n\u001b[1;32m   2530\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2531\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2533\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3674\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3675\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3677\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3729\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3730\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3731\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3732\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3733\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1663\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1665\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1666\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1143\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    693\u001b[0m                 )\n\u001b[1;32m    694\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    696\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    586\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 515\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    516\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0mkey_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m             \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_cross_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 24.12 MiB is free. Process 4520 has 14.71 GiB memory in use. Of the allocated memory 14.50 GiB is allocated by PyTorch, and 96.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import wandb\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "wandb.login(key=userdata.get('wandb'))\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    run_name=\"bert\",\n",
        "    output_dir='./results',             # output directory\n",
        "    num_train_epochs=1,                 # total number of training epochs\n",
        "    per_device_train_batch_size=16,     # batch size per device during training\n",
        "    per_device_eval_batch_size=16,      # batch size for evaluation\n",
        "    eval_strategy='epoch',        # evaluate at the end of each epoch\n",
        "    save_strategy='epoch',              # save checkpoint at the end of each epoch\n",
        "    logging_dir='./logs',               # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,        # load the best model when finished training\n",
        "    metric_for_best_model=\"accuracy\",   # use accuracy to evaluate the best model\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
        "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=classification_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "id": "lLv50bfRqNEp",
        "outputId": "a6a48d51-0bdc-4efe-9abe-7572a8556894"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99      2958\n",
            "           1       0.84      0.78      0.81       137\n",
            "\n",
            "    accuracy                           0.98      3095\n",
            "   macro avg       0.92      0.89      0.90      3095\n",
            "weighted avg       0.98      0.98      0.98      3095\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAApIAAAIjCAYAAACwHvu2AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAVtNJREFUeJzt3XmcjeX/x/H3Gcy+GYyZCWOZaOykGLKVjDWiRcjYKqJkS5IYZCS7ikqZSVT6hgotdpHti7GFGCPJjJ0xljHL+f3h53w7xjJzN8c55ryePc7j4Vz3dV/35xwNH5/ruq/bZDabzQIAAAByycXeAQAAAODeRCIJAAAAQ0gkAQAAYAiJJAAAAAwhkQQAAIAhJJIAAAAwhEQSAAAAhpBIAgAAwBASSQAAABhCIgngtg4cOKCmTZvKz89PJpNJixYtytPxDx8+LJPJpNjY2Dwd917WqFEjNWrUyN5hAMAdkUgC94CEhAS99NJLKlu2rNzd3eXr66t69epp6tSpunz5sk2vHRUVpV27dumdd97RnDlzVKtWLZte727q2rWrTCaTfH19b/o9HjhwQCaTSSaTSRMmTMj1+MeOHdPIkSMVHx+fB9ECgOMpaO8AANzekiVL9PTTT8vNzU1dunRR5cqVdfXqVa1bt06DBw/Wnj179PHHH9vk2pcvX9aGDRs0bNgw9e3b1ybXCA0N1eXLl1WoUCGbjH8nBQsW1KVLl/TDDz/omWeesTo2d+5cubu768qVK4bGPnbsmKKjo1W6dGlVr149x+f98ssvhq4HAHcbiSTgwBITE9WhQweFhoZq5cqVCg4Othzr06ePDh48qCVLltjs+idPnpQk+fv72+waJpNJ7u7uNhv/Ttzc3FSvXj19+eWX2RLJefPmqWXLlvr222/vSiyXLl2Sp6enXF1d78r1AODfYmobcGDjx49XamqqPv30U6sk8rqwsDD169fP8j4jI0OjR49WuXLl5ObmptKlS+vNN99UWlqa1XmlS5dWq1attG7dOj388MNyd3dX2bJl9fnnn1v6jBw5UqGhoZKkwYMHy2QyqXTp0pKuTQlf//U/jRw5UiaTyapt2bJleuSRR+Tv7y9vb29VqFBBb775puX4rdZIrly5UvXr15eXl5f8/f3Vpk0b7d2796bXO3jwoLp27Sp/f3/5+fmpW7duunTp0q2/2Bt07NhRP/74o86dO2dp27Jliw4cOKCOHTtm63/mzBkNGjRIVapUkbe3t3x9fdW8eXPt2LHD0mf16tV66KGHJEndunWzTJFf/5yNGjVS5cqVtXXrVjVo0ECenp6W7+XGNZJRUVFyd3fP9vkjIyNVuHBhHTt2LMefFQDyEokk4MB++OEHlS1bVnXr1s1R/549e+rtt99WzZo1NXnyZDVs2FAxMTHq0KFDtr4HDx7UU089pccff1wTJ05U4cKF1bVrV+3Zs0eS1K5dO02ePFmS9Nxzz2nOnDmaMmVKruLfs2ePWrVqpbS0NI0aNUoTJ07UE088ofXr19/2vOXLlysyMlInTpzQyJEjNWDAAP3222+qV6+eDh8+nK3/M888owsXLigmJkbPPPOMYmNjFR0dneM427VrJ5PJpAULFlja5s2bpwceeEA1a9bM1v/QoUNatGiRWrVqpUmTJmnw4MHatWuXGjZsaEnqwsPDNWrUKEnSiy++qDlz5mjOnDlq0KCBZZzTp0+refPmql69uqZMmaLGjRvfNL6pU6eqWLFiioqKUmZmpiTpo48+0i+//KLp06crJCQkx58VAPKUGYBDOn/+vFmSuU2bNjnqHx8fb5Zk7tmzp1X7oEGDzJLMK1eutLSFhoaaJZnXrl1raTtx4oTZzc3NPHDgQEtbYmKiWZL5vffesxozKirKHBoami2GESNGmP/5x8rkyZPNkswnT568ZdzXrzF79mxLW/Xq1c2BgYHm06dPW9p27NhhdnFxMXfp0iXb9bp372415pNPPmkuUqTILa/5z8/h5eVlNpvN5qeeesr82GOPmc1mszkzM9McFBRkjo6Ovul3cOXKFXNmZma2z+Hm5mYeNWqUpW3Lli3ZPtt1DRs2NEsyz5w586bHGjZsaNX2888/myWZx4wZYz506JDZ29vb3LZt2zt+RgCwJSqSgINKSUmRJPn4+OSo/9KlSyVJAwYMsGofOHCgJGVbS1mxYkXVr1/f8r5YsWKqUKGCDh06ZDjmG11fW/ndd98pKysrR+ckJSUpPj5eXbt2VUBAgKW9atWqevzxxy2f85969epl9b5+/fo6ffq05TvMiY4dO2r16tVKTk7WypUrlZycfNNpbenaukoXl2t/fGZmZur06dOWaftt27bl+Jpubm7q1q1bjvo2bdpUL730kkaNGqV27drJ3d1dH330UY6vBQC2QCIJOChfX19J0oULF3LU/88//5SLi4vCwsKs2oOCguTv768///zTqr1UqVLZxihcuLDOnj1rMOLsnn32WdWrV089e/ZU8eLF1aFDB82fP/+2SeX1OCtUqJDtWHh4uE6dOqWLFy9atd/4WQoXLixJufosLVq0kI+Pj77++mvNnTtXDz30ULbv8rqsrCxNnjxZ999/v9zc3FS0aFEVK1ZMO3fu1Pnz53N8zfvuuy9XN9ZMmDBBAQEBio+P17Rp0xQYGJjjcwHAFkgkAQfl6+urkJAQ7d69O1fn3Xizy60UKFDgpu1ms9nwNa6v37vOw8NDa9eu1fLly/X8889r586devbZZ/X4449n6/tv/JvPcp2bm5vatWunuLg4LVy48JbVSEkaO3asBgwYoAYNGuiLL77Qzz//rGXLlqlSpUo5rrxK176f3Ni+fbtOnDghSdq1a1euzgUAWyCRBBxYq1atlJCQoA0bNtyxb2hoqLKysnTgwAGr9uPHj+vcuXOWO7DzQuHCha3ucL7uxqqnJLm4uOixxx7TpEmT9Pvvv+udd97RypUrtWrVqpuOfT3O/fv3Zzu2b98+FS1aVF5eXv/uA9xCx44dtX37dl24cOGmNyhd95///EeNGzfWp59+qg4dOqhp06Zq0qRJtu8kp0l9Tly8eFHdunVTxYoV9eKLL2r8+PHasmVLno0PAEaQSAIO7PXXX5eXl5d69uyp48ePZzuekJCgqVOnSro2NSsp253VkyZNkiS1bNkyz+IqV66czp8/r507d1rakpKStHDhQqt+Z86cyXbu9Y25b9yS6Lrg4GBVr15dcXFxVonZ7t279csvv1g+py00btxYo0eP1vvvv6+goKBb9itQoEC2auc333yjv//+26rtesJ7s6Q7t4YMGaIjR44oLi5OkyZNUunSpRUVFXXL7xEA7gY2JAccWLly5TRv3jw9++yzCg8Pt3qyzW+//aZvvvlGXbt2lSRVq1ZNUVFR+vjjj3Xu3Dk1bNhQmzdvVlxcnNq2bXvLrWWM6NChg4YMGaInn3xSr776qi5duqQZM2aofPnyVjebjBo1SmvXrlXLli0VGhqqEydO6MMPP1SJEiX0yCOP3HL89957T82bN1dERIR69Oihy5cva/r06fLz89PIkSPz7HPcyMXFRW+99dYd+7Vq1UqjRo1St27dVLduXe3atUtz585V2bJlrfqVK1dO/v7+mjlzpnx8fOTl5aXatWurTJkyuYpr5cqV+vDDDzVixAjLdkSzZ89Wo0aNNHz4cI0fPz5X4wFAXqEiCTi4J554Qjt37tRTTz2l7777Tn369NEbb7yhw4cPa+LEiZo2bZql76xZsxQdHa0tW7botdde08qVKzV06FB99dVXeRpTkSJFtHDhQnl6eur1119XXFycYmJi1Lp162yxlypVSp999pn69OmjDz74QA0aNNDKlSvl5+d3y/GbNGmin376SUWKFNHbb7+tCRMmqE6dOlq/fn2ukzBbePPNNzVw4ED9/PPP6tevn7Zt26YlS5aoZMmSVv0KFSqkuLg4FShQQL169dJzzz2nNWvW5OpaFy5cUPfu3VWjRg0NGzbM0l6/fn3169dPEydO1MaNG/PkcwFAbpnMuVmNDgAAAPw/KpIAAAAwhEQSAAAAhpBIAgAAwBASSQAAABhCIgkAAABDSCQBAABgCIkkAAAADMmXT7bxqNHX3iEAsJGzW963dwgAbMTdjlmJLXOHy9vz759bVCQBAABgSL6sSAIAAOSKidqaESSSAAAAJpO9I7gnkX4DAADAECqSAAAATG0bwrcGAAAAQ6hIAgAAsEbSECqSAAAAMISKJAAAAGskDeFbAwAAgCFUJAEAAFgjaQiJJAAAAFPbhvCtAQAAwBAqkgAAAExtG0JFEgAAAIZQkQQAAGCNpCF8awAAADCEiiQAAABrJA2hIgkAAABDqEgCAACwRtIQEkkAAACmtg0h/QYAAIAhVCQBAACY2jaEbw0AAACGUJEEAACgImkI3xoAAAAMoSIJAADgwl3bRlCRBAAAgCFUJAEAAFgjaQiJJAAAABuSG0L6DQAAAEOoSAIAADC1bQjfGgAAAAyhIgkAAMAaSUOoSAIAAMAQKpIAAACskTSEbw0AAACGUJEEAABgjaQhJJIAAABMbRvCtwYAAABDqEgCAAAwtW0IFUkAAAAYQkUSAACANZKG8K0BAADAECqSAAAArJE0hIokAAAADKEiCQAAwBpJQ0gkAQAASCQN4VsDAACAIVQkAQAAuNnGECqSAAAAMISKJAAAAGskDeFbAwAAgCFUJAEAAFgjaQgVSQAAABhCRRIAAIA1koaQSAIAADC1bQjpNwAAAAyhIgkAAJyeiYqkIVQkAQAAYAgVSQAA4PSoSBpDRRIAAACGUJEEAACgIGkIFUkAAAAY4hCJ5KhRo3Tp0qVs7ZcvX9aoUaPsEBEAAHAmJpPJZq/ciImJ0UMPPSQfHx8FBgaqbdu22r9/v1WfRo0aZbtGr169rPocOXJELVu2lKenpwIDAzV48GBlZGRY9Vm9erVq1qwpNzc3hYWFKTY2Ntffm0MkktHR0UpNTc3WfunSJUVHR9shIgAA4EwcJZFcs2aN+vTpo40bN2rZsmVKT09X06ZNdfHiRat+L7zwgpKSkiyv8ePHW45lZmaqZcuWunr1qn777TfFxcUpNjZWb7/9tqVPYmKiWrZsqcaNGys+Pl6vvfaaevbsqZ9//jlX8TrEGkmz2XzTL3rHjh0KCAiwQ0QAAAB3308//WT1PjY2VoGBgdq6dasaNGhgaff09FRQUNBNx/jll1/0+++/a/ny5SpevLiqV6+u0aNHa8iQIRo5cqRcXV01c+ZMlSlTRhMnTpQkhYeHa926dZo8ebIiIyNzHK9dK5KFCxdWQECATCaTypcvr4CAAMvLz89Pjz/+uJ555hl7hggAAJyALSuSaWlpSklJsXqlpaXlKK7z589LUrbC2ty5c1W0aFFVrlxZQ4cOtVoiuGHDBlWpUkXFixe3tEVGRiolJUV79uyx9GnSpInVmJGRkdqwYUOuvje7ViSnTJkis9ms7t27Kzo6Wn5+fpZjrq6uKl26tCIiIuwYIQAAwL8TExOTbaneiBEjNHLkyNuel5WVpddee0316tVT5cqVLe0dO3ZUaGioQkJCtHPnTg0ZMkT79+/XggULJEnJyclWSaQky/vk5OTb9klJSdHly5fl4eGRo89m10QyKipKklSmTBnVrVtXhQoVsmc4AADASdlyQ/KhQ4dqwIABVm1ubm53PK9Pnz7avXu31q1bZ9X+4osvWn5dpUoVBQcH67HHHlNCQoLKlSuXN0HnkEOskWzYsKGysrL0xx9/6MSJE8rKyrI6/s81AQAAAPcSNze3HCWO/9S3b18tXrxYa9euVYkSJW7bt3bt2pKkgwcPqly5cgoKCtLmzZut+hw/flySLOsqg4KCLG3/7OPr65vjaqTkIInkxo0b1bFjR/35558ym81Wx0wmkzIzM+0UGQAAcAoOsiG52WzWK6+8ooULF2r16tUqU6bMHc+Jj4+XJAUHB0uSIiIi9M477+jEiRMKDAyUJC1btky+vr6qWLGipc/SpUutxlm2bFmulxQ6RCLZq1cv1apVS0uWLFFwcDDPuwQAAE6pT58+mjdvnr777jv5+PhY1jT6+fnJw8NDCQkJmjdvnlq0aKEiRYpo586d6t+/vxo0aKCqVatKkpo2baqKFSvq+eef1/jx45WcnKy33npLffr0sVRGe/Xqpffff1+vv/66unfvrpUrV2r+/PlasmRJruI1mW8sAdqBl5eXduzYobCwsDwZz6NG3zwZB4DjObvlfXuHAMBG3O1Y3vLv9IXNxj43t3OO+96qmDZ79mx17dpVf/31lzp37qzdu3fr4sWLKlmypJ588km99dZb8vX1tfT/888/1bt3b61evVpeXl6KiorSuHHjVLDg/77k1atXq3///vr9999VokQJDR8+XF27ds3VZ3OIimTt2rV18ODBPEskAQAA7kV3qu+VLFlSa9asueM4oaGh2aaub9SoUSNt3749V/HdyCESyVdeeUUDBw5UcnKyqlSpku3u7eulWgAAAFtgWZ0xDpFItm/fXpLUvXt3S5vJZLI88YabbQAAgC2RSBrjEIlkYmKivUMAAABALjlEIhkaGmrvEAAAgBOjImmMXZ+1/U9z5sxRvXr1FBISoj///FPStUcofvfdd3aODAAAADfjEInkjBkzNGDAALVo0ULnzp2zrIn09/fXlClT7BscAADI/0w2fOVjDpFITp8+XZ988omGDRumAgUKWNpr1aqlXbt22TEyAAAA3IpDrJFMTExUjRo1srW7ubnp4sWLdogIAAA4E9ZIGuMQFckyZcpYnhP5Tz/99JPCw8PvfkAAAAC4I4eoSA4YMEB9+vTRlStXZDabtXnzZn355ZeKiYnRrFmz7B0eAADI56hIGuMQiWTPnj3l4eGht956S5cuXVLHjh0VEhKiqVOnqkOHDvYODwAA5HMkksY4RCIpSZ06dVKnTp106dIlpaamKjAw0N4hAQAA4DYcJpG8ztPTU56envYOAwAAOBMKkobYLZGsUaNGjsvI27Zts3E0AAAAyC27JZJt27a116UBAACssEbSGLslkiNGjLDXpQEAAJAHHG6NJAAAwN1GRdIYh0gkXVxcbvsbeP3Z2wAAAHAcDpFILly40Op9enq6tm/frri4OEVHR9spKgAA4CyoSBrjEIlkmzZtsrU99dRTqlSpkr7++mv16NHDDlEBAABnQSJpjEM8a/tW6tSpoxUrVtg7DAAAANyEQ1Qkb+by5cuaNm2a7rvvPnuHAgAA8jsKkoY4RCJZuHBhq5Ky2WzWhQsX5OnpqS+++MKOkQEAAOBWHCKRnDx5slUi6eLiomLFiql27doqXLiwHSMDAADOgDWSxjhEItm1a1d7hwAAAIBcslsiuXPnzhz3rVq1qg0jAQAAzo6KpDF2SySrV68uk8kks9ks6fa/gWxIDgAA4HjslkgmJiZafr19+3YNGjRIgwcPVkREhCRpw4YNmjhxosaPH2+vEAEAgJOgImmM3RLJ0NBQy6+ffvppTZs2TS1atLC0Va1aVSVLltTw4cPVtm1bO0QIAACcBnmkIQ6xIfmuXbtUpkyZbO1lypTR77//boeIAAAAcCcOkUiGh4crJiZGV69etbRdvXpVMTExCg8Pt2NkAADAGZhMJpu98jOH2P5n5syZat26tUqUKGG5Q3vnzp0ymUz64Ycf7BwdAAAAbsYhEsmHH35Yhw4d0ty5c7Vv3z5J0rPPPquOHTvKy8vLztEBAID8Lr9XDm3FIRJJSfLy8tKLL75o7zAAAACQQw6RSH7++ee3Pd6lS5e7FAnsYVD3pmr7aDWVL11cl9PStWnHIQ2b+p0O/HnC0qdMiaIa1/9JRdQoK7dCBbXst70a8O43OnHmgqXPN1NeUrXy96lYgI/OplzSqk379da075R08rylT5OIcA3v1ULh5YJ15Wq61m9L0JCJC3Qk6cxd/cwA/ufTTz7SimW/KDHxkNzc3VW9eg29NmCQSpcpa+mTlpamiePH6acfl+rq1auqW+8RDRs+QkWKFrVj5MhPqEgaYzJf3xHcjm58nnZ6erouXbokV1dXeXp66syZ3P0l71Gjb16GBxv77v2X9c3PW7V1z58qWLCAovu2VqWwENVoN0aXrlyVp7urtswfql1//K3RM5dKkka83FLBxfzUoMtEy6b2r3RqrE07E5V86rxCAv0V0/9JSVLjrpMkSaEhRRS/4C1N+2KlYhdtkJ+3u8YPai9vT3fV7fiufT48cu3slvftHQLyWO8Xe6hZ85aqVKWKMjMyNX3qJB08cEALvl8iT09PSdKYUSP065o1GvVOjHx8fBTzzmi5mEyKm/uVnaNHXnK3Y3mrdL/FNhv78NRWNhvb3hyiInn27NlsbQcOHFDv3r01ePBgO0SEu6lN3w+t3r844gv9tXKcalQsqfXbEhRRvaxCQ4qoznPv6sLFK5Kknm/PUdKa8Wr0cHmt2rRfkjR97irLGEeSzmrC7GWaP+kFFSzoooyMLNWsWFIFXFw08oPFluRzyucr9M3kFy19ANx9Mz7+1Or9qHfGqXH9CO39fY8erPWQLly4oIXffqtx4yeodp1rD60YNWas2rZuoZ074lW1WnU7RI38hoqkMQ6x/c/N3H///Ro3bpz69etn71Bwl/l6u0uSzp6/JElycy0os9mstKsZlj5X0jKUlWVW3erlbjpGYV9PdWheSxt3JFoSxG2//6Usc5a6tKkjFxeTfL3d1bHlw1q5aT9JJOBAUi9cW7Li6+cnSfp9z25lZKSrdkRdS58yZcspODhEO+Lj7REi8iOTDV/5mMMmkpJUsGBBHTt27LZ90tLSlJKSYvUyZ/Fs7nuVyWTSe4Oe0m/bE/R7QpIkafOuw7p4+are6ddGHu6F5OnuqnEDnlTBggUUVNTX6vwxr7bRqd8m6tia8SoZHKCn+39sOfbnsdNq9fIHiu7bWuc3TdHxXyfovuL+6vz6Z3f1MwK4taysLI1/d6yq16ip++8vL0k6feqUChUqJF9f65/3gCJFdOrUSXuECeD/OcTU9vfff2/13mw2KykpSe+//77q1at323NjYmIUHR1t1Vag+EMqFPxwnscJ25sy9BlVCgvWY90mW9pOnU1Vp9c/1bQ3n9XLzzVUVpZZ83/aqm2/H1HWDUt8J3++XLGLNqhUcICGvdRcs0Y/r3avzpQkFS/iow+Hd9TcHzZp/k9b5e3lprd7t9K8CT3Ushfr7gBHMHZMtBIOHFDsnHn2DgVOhqltYxwikbzxWdomk0nFihXTo48+qokTJ9723KFDh2rAgAFWbYH1h+R1iLgLJg95Wi3qV1aTHlP094lzVsdWbNynSk9Eq4i/lzIysnQ+9bISl43V4Z+3WvU7fe6iTp+7qINHTmh/YrIO/jxGtauW0aadiXrp2QZKSb2sYVO/s/TvPixOB38eo4erlNbmXYfvwqcEcCtjx4zS2jWr9VncFyoeFGRpL1K0qNLT05WSkmJVlTxz+rSKFi1mj1AB/D+HSCSzsoyvT3Nzc5Obm5tVm8mlwL8NCXfZ5CFP64lHq6npC1P157HTt+x3+txFSVLDh8orMMBbi9fsumVfF5dr/7p0LXTtf3NPd1dlZVlXMDP///+9630B3H1ms1kx74zWyhXL9GnsHJUoUdLqeMVKlVWwYCFt3rhBTZpGSpIOJx5SUtIxVate3Q4RIz+iImmMQySScG5Thj6jZ5vX0tP9P1bqxSsqXsRHknQ+9YqupKVLkp5/oo72Jybr5NlU1a5aRhMGP6Xpc1dZ9pp8qHKoHqwUqt+2J+jchUsqU6KYRrzcUglHTmrTzkRJ0o+/7tErnRpr6IvNNP+nrfLxdFN03yf057HTit931D4fHoDGjo7Wj0sXa8r0D+Xl6aVTJ6+te/T28ZG7u7t8fHz0ZPv2mjB+nHz9/OTt7a1xY8eoWvUa3LEN2JlD7CMpSUePHtX333+vI0eO6OrVq1bHJk2alKux2Efy3nJ5+83XJ77w9hx98cMmSdLoV59Q59Z1FODnqT+PndGs/6zTtC9WWvpWCgvRhMHtVaV8CXl5uCr51Hn98ttevfvJTzr2jw3Jn458UP2jmuj+0EBdunJVm3Ym6q2p3+mPw8dt+yGRZ9hHMv+pVqnCTdtHjYlRmyfbSfrfhuQ/Ll2iq+n/vyH5WyNUtBhT2/mJPfeRDBv0o83GPjihuc3GtjeHSCRXrFihJ554QmXLltW+fftUuXJlHT58WGazWTVr1tTKlSvvPMg/kEgC+ReJJJB/kUjeexxi+5+hQ4dq0KBB2rVrl9zd3fXtt9/qr7/+UsOGDfX000/bOzwAAJDPmUwmm73yM4dIJPfu3Wt5nnbBggV1+fJleXt7a9SoUXr3XR5dBwAAbMtkst0rP3OIRNLLy8uyLjI4OFgJCQmWY6dOnbJXWAAAALgNh7hru06dOlq3bp3Cw8PVokULDRw4ULt27dKCBQtUp04de4cHAADyufw+BW0rDpFITpo0SampqZKk6Ohopaam6uuvv9b999+f6zu2AQAAcHfYPZHMzMzU0aNHVbVqVUnXprlnzpxp56gAAIAzoSBpjN3XSBYoUEBNmzbV2bNn7R0KAAAAcsHuFUlJqly5sg4dOqQyZcrYOxQAAOCEeFSuMXavSErSmDFjNGjQIC1evFhJSUlKSUmxegEAAMDxOERFskWLFpKkJ554wuquKbPZLJPJpMzMTHuFBgAAnABrJI1xiERy1apV9g4BAAA4Mbb/McauiWSXLl30wQcfqGHDhpKkHTt2qGLFiipUqJA9wwIAAEAO2HWN5Ny5c3X58mXL+/r16+uvv/6yY0QAAMAZ8YhEY+yaSJrN5tu+BwAAgONyiDWSAAAA9sQaSWPsnkj+/vvvSk5OlnStIrlv3z7L4xKvu/7UGwAAADgOuyeSjz32mNWUdqtWrSRd+5cB2/8AAIC7gYqkMXZNJBMTE+15eQAAAPwLdk0kQ0ND7Xl5AAAASfn/7mpbcYhHJP5TlSpV2AIIAADcVSaTyWav/MzhEsnDhw8rPT3d3mEAAADgDux+sw0AAIC95fPCoc04XEWyfv368vDwsHcYAAAAuAOHSySXLl2q4OBge4cBAACciKOskYyJidFDDz0kHx8fBQYGqm3bttq/f79VnytXrqhPnz4qUqSIvL291b59ex0/ftyqz5EjR9SyZUt5enoqMDBQgwcPVkZGhlWf1atXq2bNmnJzc1NYWJhiY2Nz/b05zNT2gQMHtGrVKp04cUJZWVlWx95++207RQUAAHD3rFmzRn369NFDDz2kjIwMvfnmm2ratKl+//13eXl5SZL69++vJUuW6JtvvpGfn5/69u2rdu3aaf369ZKkzMxMtWzZUkFBQfrtt9+UlJSkLl26qFChQho7dqyka1swtmzZUr169dLcuXO1YsUK9ezZU8HBwYqMjMxxvCazAzzg+pNPPlHv3r1VtGhRBQUFWWXvJpNJ27Zty9V4HjX65nWIABzE2S3v2zsEADbibsfyVq0xq2w29n/famz43JMnTyowMFBr1qxRgwYNdP78eRUrVkzz5s3TU089JUnat2+fwsPDtWHDBtWpU0c//vijWrVqpWPHjql48eKSpJkzZ2rIkCE6efKkXF1dNWTIEC1ZskS7d++2XKtDhw46d+6cfvrppxzH5xBT22PGjNE777yj5ORkxcfHa/v27ZZXbpNIAAAAR5KWlqaUlBSrV1paWo7OPX/+vCQpICBAkrR161alp6erSZMmlj4PPPCASpUqpQ0bNkiSNmzYoCpVqliSSEmKjIxUSkqK9uzZY+nzzzGu97k+Rk45RCJ59uxZPf300/YOAwAAOClbrpGMiYmRn5+f1SsmJuaOMWVlZem1115TvXr1VLlyZUlScnKyXF1d5e/vb9W3ePHiSk5OtvT5ZxJ5/fj1Y7frk5KSosuXL+f4e3OIRPLpp5/WL7/8Yu8wAAAA8tzQoUN1/vx5q9fQoUPveF6fPn20e/duffXVV3chSmMc4mabsLAwDR8+XBs3blSVKlVUqFAhq+OvvvqqnSIDAADOwJb7SLq5ucnNzS1X5/Tt21eLFy/W2rVrVaJECUt7UFCQrl69qnPnzllVJY8fP66goCBLn82bN1uNd/2u7n/2ufFO7+PHj8vX1zdX2zA6RCL58ccfy9vbW2vWrNGaNWusjplMJhJJAABgU47yKEOz2axXXnlFCxcu1OrVq1WmTBmr4w8++KAKFSqkFStWqH379pKk/fv368iRI4qIiJAkRURE6J133tGJEycUGBgoSVq2bJl8fX1VsWJFS5+lS5dajb1s2TLLGDnlEIlkYmKivUMAAACwuz59+mjevHn67rvv5OPjY1nT6OfnJw8PD/n5+alHjx4aMGCAAgIC5Ovrq1deeUURERGqU6eOJKlp06aqWLGinn/+eY0fP17Jycl666231KdPH0tltFevXnr//ff1+uuvq3v37lq5cqXmz5+vJUuW5Cpeh0gk/+n6bkSO8i8DAACQ/zlK2jFjxgxJUqNGjazaZ8+era5du0qSJk+eLBcXF7Vv315paWmKjIzUhx9+aOlboEABLV68WL1791ZERIS8vLwUFRWlUaNGWfqUKVNGS5YsUf/+/TV16lSVKFFCs2bNytUekpKD7CMpSZ9//rnee+89HThwQJJUvnx5DR48WM8//3yux2IfSSD/Yh9JIP+y5z6SdcatuXMngza+0dBmY9ubQ1QkJ02apOHDh6tv376qV6+eJGndunXq1auXTp06pf79+9s5QgAAkJ8xE2qMQySS06dP14wZM9SlSxdL2xNPPKFKlSpp5MiRJJIAAAAOyCESyaSkJNWtWzdbe926dZWUlGSHiAAAgDOhIGmMQ2xIHhYWpvnz52dr//rrr3X//ffbISIAAADciUNUJKOjo/Xss89q7dq1ljWS69ev14oVK26aYAIAAOQl1kga4xCJZPv27bVp0yZNmjRJixYtkiSFh4dr8+bNqlGjhn2DAwAA+R55pDEOkUhK13Zqnzt3rr3DAAAAQA7ZNZF0cXG5YynZZDIpIyPjLkUEAACcEVPbxtg1kVy4cOEtj23YsEHTpk1TVlbWXYwIAAAAOWXXRLJNmzbZ2vbv36833nhDP/zwgzp16mT1OB8AAABboCJpjENs/yNJx44d0wsvvKAqVaooIyND8fHxiouLU2hoqL1DAwAAwE3YPZE8f/68hgwZorCwMO3Zs0crVqzQDz/8oMqVK9s7NAAA4CRMJtu98jO7Tm2PHz9e7777roKCgvTll1/edKobAAAAjsmuieQbb7whDw8PhYWFKS4uTnFxcTftt2DBgrscGQAAcCaskTTGrolkly5d+I0DAAB2RzpijF0TydjYWHteHgAAAP+CwzzZBgAAwF6YITXG7ndtAwAA4N5ERRIAADg9CpLGUJEEAACAIVQkAQCA03OhJGkIFUkAAAAYQkUSAAA4PQqSxpBIAgAAp8f2P8YwtQ0AAABDqEgCAACn50JB0hAqkgAAADCEiiQAAHB6rJE0hookAAAADKEiCQAAnB4FSWOoSAIAAMAQKpIAAMDpmURJ0ggSSQAA4PTY/scYprYBAABgCBVJAADg9Nj+xxgqkgAAADCEiiQAAHB6FCSNoSIJAAAAQ6hIAgAAp+dCSdIQKpIAAAAwhIokAABwehQkjSGRBAAATo/tf4xhahsAAACGUJEEAABOj4KkMVQkAQAAYAgVSQAA4PTY/scYKpIAAAAwhIokAABwetQjjaEiCQAAAEOoSAIAAKfHPpLGkEgCAACn50IeaQhT2wAAADCEiiQAAHB6TG0bQ0USAAAAhlCRBAAATo+CpDFUJAEAAGAIFUkAAOD0WCNpDBVJAAAAGEJFEgAAOD32kTSGRBIAADg9praNYWobAAAAhlCRBAAATo96pDFUJAEAAGCIoUTy119/VefOnRUREaG///5bkjRnzhytW7cuT4MDAAC4G1xMJpu98rNcJ5LffvutIiMj5eHhoe3btystLU2SdP78eY0dOzbPAwQAAIBjynUiOWbMGM2cOVOffPKJChUqZGmvV6+etm3blqfBAQAA3A0mk+1e+VmuE8n9+/erQYMG2dr9/Px07ty5vIgJAAAA94BcJ5JBQUE6ePBgtvZ169apbNmyeRIUAADA3WQymWz2ys9ynUi+8MIL6tevnzZt2iSTyaRjx45p7ty5GjRokHr37m2LGAEAAOCAcp1IvvHGG+rYsaMee+wxpaamqkGDBurZs6deeuklvfLKK7aIEQAAwKYcaY3k2rVr1bp1a4WEhMhkMmnRokVWx7t27Zqt6tmsWTOrPmfOnFGnTp3k6+srf39/9ejRQ6mpqVZ9du7cqfr168vd3V0lS5bU+PHjcx1rrjckN5lMGjZsmAYPHqyDBw8qNTVVFStWlLe3d64vDgAA4AgcaZueixcvqlq1aurevbvatWt30z7NmjXT7NmzLe/d3Nysjnfq1ElJSUlatmyZ0tPT1a1bN7344ouaN2+eJCklJUVNmzZVkyZNNHPmTO3atUvdu3eXv7+/XnzxxRzHavjJNq6urqpYsaLR0wEAAJxCWlqaZbvE69zc3LIlf9c1b95czZs3v+2Ybm5uCgoKuumxvXv36qefftKWLVtUq1YtSdL06dPVokULTZgwQSEhIZo7d66uXr2qzz77TK6urqpUqZLi4+M1adKkXCWSuZ7abty4sR599NFbvgAAAO41tpzajomJkZ+fn9UrJibmX8W7evVqBQYGqkKFCurdu7dOnz5tObZhwwb5+/tbkkhJatKkiVxcXLRp0yZLnwYNGsjV1dXSJzIyUvv379fZs2dzHEeuK5LVq1e3ep+enq74+Hjt3r1bUVFRuR0OAAAgXxs6dKgGDBhg1XaramRONGvWTO3atVOZMmWUkJCgN998U82bN9eGDRtUoEABJScnKzAw0OqcggULKiAgQMnJyZKk5ORklSlTxqpP8eLFLccKFy6co1hynUhOnjz5pu0jR47MtogTAADgXmDLbXpuN41tRIcOHSy/rlKliqpWrapy5cpp9erVeuyxx/LsOjlh6FnbN9O5c2d99tlneTUcAAAAcqBs2bIqWrSoZZ/voKAgnThxwqpPRkaGzpw5Y1lXGRQUpOPHj1v1uf7+Vmsvb8bwzTY32rBhg9zd3fNquH/l7Jb37R0CABtJz8iydwgAbMS9YJ7Vt3LNflf+944eParTp08rODhYkhQREaFz585p69atevDBByVJK1euVFZWlmrXrm3pM2zYMKWnp1seeb1s2TJVqFAhx9PakoFE8sbb0M1ms5KSkvTf//5Xw4cPz+1wAAAA+IfU1FSrpwgmJiYqPj5eAQEBCggIUHR0tNq3b6+goCAlJCTo9ddfV1hYmCIjIyVJ4eHhatasmV544QXNnDlT6enp6tu3rzp06KCQkBBJUseOHRUdHa0ePXpoyJAh2r17t6ZOnXrLJYy3YjKbzebcnNCtWzer9y4uLipWrJgeffRRNW3aNFcXt5UrGfaOAICtUJEE8i8fd/vVBV9dtM9mY09r+0Cu+q9evVqNGzfO1h4VFaUZM2aobdu22r59u86dO6eQkBA1bdpUo0ePttwsI13bkLxv37764Ycf5OLiovbt22vatGlW+37v3LlTffr00ZYtW1S0aFG98sorGjJkSK5izVUimZmZqfXr16tKlSq5KnvebSSSQP5FIgnkX/ZMJF/7znaJ5JQ2uUsk7yW5+h0rUKCAmjZtqnPnztkoHAAAANwrcp36V65cWYcOHbJFLAAAAHbhYrLdKz/LdSI5ZswYDRo0SIsXL1ZSUpJSUlKsXgAAAHAOOb5re9SoURo4cKBatGghSXriiSesNu80m80ymUzKzMzM+ygBAABsyJYbkudnOU4ko6Oj1atXL61atcqW8QAAAOAekeNE8vrN3Q0bNrRZMAAAAPaQ39cy2kqu1khS9gUAAMB1uXqyTfny5e+YTJ45c+ZfBQQAAHC3USszJleJZHR0tPz8/GwVCwAAgF24kEkakqtEskOHDgoMDLRVLAAAALiH5DiRZH0kAADIr+z3cMZ7W46/t1w8khsAAABOIMcVyaysLFvGAQAAYDdMvBpDJRcAAACG5OpmGwAAgPyIu7aNoSIJAAAAQ6hIAgAAp0dB0hgSSQAA4PR41rYxTG0DAADAECqSAADA6XGzjTFUJAEAAGAIFUkAAOD0KEgaQ0USAAAAhlCRBAAATo+7to2hIgkAAABDqEgCAACnZxIlSSNIJAEAgNNjatsYprYBAABgCBVJAADg9KhIGkNFEgAAAIZQkQQAAE7PxI7khlCRBAAAgCFUJAEAgNNjjaQxVCQBAABgCBVJAADg9FgiaQyJJAAAcHouZJKGMLUNAAAAQ6hIAgAAp8fNNsZQkQQAAIAhVCQBAIDTY4mkMVQkAQAAYAgVSQAA4PRcREnSCCqSAAAAMISKJAAAcHqskTSGRBIAADg9tv8xhqltAAAAGEJFEgAAOD0ekWgMFUkAAAAYQkUSAAA4PQqSxlCRBAAAgCFUJAEAgNNjjaQxVCQBAABgCBVJAADg9ChIGkMiCQAAnB5TtMbwvQEAAMAQKpIAAMDpmZjbNoSKJAAAAAyhIgkAAJwe9UhjqEgCAADAECqSAADA6bEhuTFUJAEAAGAIFUkAAOD0qEcaQyIJAACcHjPbxjC1DQAAAEOoSAIAAKfHhuTGUJEEAACAIVQkAQCA06OyZgzfGwAAAAwhkQQAAE7PZDLZ7JVba9euVevWrRUSEiKTyaRFixZZHTebzXr77bcVHBwsDw8PNWnSRAcOHLDqc+bMGXXq1Em+vr7y9/dXjx49lJqaatVn586dql+/vtzd3VWyZEmNHz8+17GSSAIAADiQixcvqlq1avrggw9uenz8+PGaNm2aZs6cqU2bNsnLy0uRkZG6cuWKpU+nTp20Z88eLVu2TIsXL9batWv14osvWo6npKSoadOmCg0N1datW/Xee+9p5MiR+vjjj3MVq8lsNpuNfUzHdSXD3hEAsJX0jCx7hwDARnzc7Vff+ib+mM3Gfrp6iOFzTSaTFi5cqLZt20q6Vo0MCQnRwIEDNWjQIEnS+fPnVbx4ccXGxqpDhw7au3evKlasqC1btqhWrVqSpJ9++kktWrTQ0aNHFRISohkzZmjYsGFKTk6Wq6urJOmNN97QokWLtG/fvhzHR0USAADAhtLS0pSSkmL1SktLMzRWYmKikpOT1aRJE0ubn5+fateurQ0bNkiSNmzYIH9/f0sSKUlNmjSRi4uLNm3aZOnToEEDSxIpSZGRkdq/f7/Onj2b43hIJAEAgNOz5RrJmJgY+fn5Wb1iYmIMxZmcnCxJKl68uFV78eLFLceSk5MVGBhodbxgwYIKCAiw6nOzMf55jZxg+x8AAOD0bFlZGzp0qAYMGGDV5ubmZsMr3j0kkgAAADbk5uaWZ4ljUFCQJOn48eMKDg62tB8/flzVq1e39Dlx4oTVeRkZGTpz5ozl/KCgIB0/ftyqz/X31/vkBFPbAADA6TnS9j+3U6ZMGQUFBWnFihWWtpSUFG3atEkRERGSpIiICJ07d05bt2619Fm5cqWysrJUu3ZtS5+1a9cqPT3d0mfZsmWqUKGCChcunON4SCQBAAAcSGpqquLj4xUfHy/p2g028fHxOnLkiEwmk1577TWNGTNG33//vXbt2qUuXbooJCTEcmd3eHi4mjVrphdeeEGbN2/W+vXr1bdvX3Xo0EEhIdfuIO/YsaNcXV3Vo0cP7dmzR19//bWmTp2abQr+Ttj+B8A9he1/gPzLntv/LNqZ8xtMcqtt1ZxPFUvS6tWr1bhx42ztUVFRio2Nldls1ogRI/Txxx/r3LlzeuSRR/Thhx+qfPnylr5nzpxR37599cMPP8jFxUXt27fXtGnT5O3tbemzc+dO9enTR1u2bFHRokX1yiuvaMiQIbmK1SESyQIFCigpKSnbHUanT59WYGCgMjMzczUeiSSQf5FIAvkXieS9xyFutrlVLpuWlma1vxEAAIAt5PFSRqdh10Ry2rRpkq4tcJ01a5ZVuTUzM1Nr167VAw88YK/wAAAAcBt2TSQnT54s6VpFcubMmSpQoIDlmKurq0qXLq2ZM2faKzwAAOAkXERJ0gi7JpKJiYmSpMaNG2vBggW5ut0cAAAgrzC1bYxDrJFctWqVvUMAAABALjlEIpmZmanY2FitWLFCJ06cUFaW9V2ZK1eutFNkAADAGZiY2jbEIRLJfv36KTY2Vi1btlTlypXzfBd4AAAA5D2HSCS/+uorzZ8/Xy1atLB3KAAAwAlRwzLGIR6R6OrqqrCwMHuHAQAAgFxwiERy4MCBmjp16i03JgcAALAlF5ls9srPHGJqe926dVq1apV+/PFHVapUSYUKFbI6vmDBAjtFBgAAgFtxiETS399fTz75pL3DAAAAToo1ksY4RCI5e/Zse4cAAACcGImkMQ6xRlKSMjIytHz5cn300Ue6cOGCJOnYsWNKTU21c2QAAAC4GYeoSP75559q1qyZjhw5orS0ND3++OPy8fHRu+++q7S0NJ63DQAAbIoNyY1xiIpkv379VKtWLZ09e1YeHh6W9ieffFIrVqywY2QAAAC4FYeoSP7666/67bff5OrqatVeunRp/f3333aKCgAAOAsXCpKGOERFMisrS5mZmdnajx49Kh8fHztEBAAAgDtxiESyadOmmjJliuW9yWRSamqqRowYwWMTAQCAzZls+F9+ZjI7wONkjh49qsjISJnNZh04cEC1atXSgQMHVLRoUa1du1aBgYG5Gu9Kho0CBWB36RlZ9g4BgI34uNuvvrVy32mbjf3oA0VsNra9OUQiKV3b/uerr77Szp07lZqaqpo1a6pTp05WN9/kFIkkkH+RSAL5lz0TyVX7bZdINq6QfxNJh7jZRpIKFiyozp072zsMAADghPL7FLSt2C2R/P7773Pc94knnrBhJAAAADDCbolk27Ztc9TPZDLd9I5uAACAvML2P8bYLZHMymKdEwAAwL3MYdZIAgAA2AtrJI1xiERy1KhRtz3+9ttv36VIAAAAkFMOsf1PjRo1rN6np6crMTFRBQsWVLly5bRt27Zcjcf2P/nP/K/maf7XX+rY/z8ys1zY/Xqp98t6pH5DSVJaWpomjh+nn35cqqtXr6puvUc0bPgIFSla1J5hwwbY/ufet23rFs2J/Ux79+7RqZMnNWHydDV6tInluNls1kcfTtfCBd8o9cIFVateQ28MG6FSoaUlSf/dslm9ekbddOy4ufNVqXKVu/ExYAP23P5n3YGzNhv7kfsL22xse3OIiuT27duztaWkpKhr16568skn7RARHE1g8SD16z9IpUJDZTab9cN3i9Svbx99/e1ChYXdr/feHatf16zRe5OmyMfHRzHvjNaAfn0VN/cre4cO4AaXL1/W/RUq6Im27TR4wKvZjsfNnqWvvvxCI0fH6L77SmjGB9P0Su8XNH/hYrm5uala9er6acVaq3NmfjBNWzZtVMVKle/WxwAgB6lI3squXbvUunVrHT58OFfnUZF0DvUjHlb/QYP1eNNmavRIhMaNn6DHI5tJkhIPJaht6xaaM+9rVa1W3b6BIk9RkcxfalULt6pIms1mNWvSQJ27dNPzUd0lSakXLqjpo49oxKiximzeMtsYGenpav54Iz37XCf1fOnluxo/8pY9K5LrbViRrJePK5IO8aztWzl//rzOnz9v7zDgYDIzM/Xj0iW6fPmSqlWrod/37FZGRrpqR9S19ClTtpyCg0O0Iz7efoECyLW//z6q06dO6eHaEZY2bx8fVa5SVbt27rjpOWvWrNL58+fUum27uxUm8iEXk8lmr/zMIaa2p02bZvXebDYrKSlJc+bMUfPmzW97blpamtLS0qzPL+AmNze3PI8T9nXgj/16vmMHXb2aJk9PT02e9oHKhYVp/769KlSokHx9fa36BxQpolOnTtopWgBGnD51SpJUpIj1I+UCihTV6Vv8PH+38D+qU7eeihcPsnl8AKw5RCI5efJkq/cuLi4qVqyYoqKiNHTo0NueGxMTo+joaKu2YcNH6K23R+Z1mLCz0qXLaP63i5SaekHLfvlZw98cok9jv7B3WADs6PjxZG38bb1i3pt8587AbeTvuqHtOEQimZiYaPjcoUOHasCAAVZt5gJUI/OjQq6uKhUaKkmqWKmy9uzepblffK7IZs2Vnp6ulJQUq6rkmdOnVbRoMXuFC8CA6zstnD59WkWLBVraz5w+pfIVwrP1/2HRAvn5+athw8Z3LUYA/+PQayRzws3NTb6+vlYvprWdQ1ZWltKvXlXFSpVVsGAhbd64wXLscOIhJSUdU7Xq1e0XIIBcu+++EipStKi2bNpoaUtNTdXuXTtVpWo1q77XdnBYqJat26hgoUJ3O1TkNyYbvvIxu1Uk27XL+aLoBQsW2DAS3AumTp6oR+o3UFBwsC5dvKilSxbrv1s2a8bHn8rHx0dPtm+vCePHydfPT97e3ho3doyqVa/BHduAA7p06aL+OnLE8v7vv49q/7698vPzU1BwiJ7r1EWffjJTJUNDLdv/FCsWaLXXpCRt2bxRf/99VG3bPXW3PwKA/2e3RNLPz8/ya7PZrIULF8rPz0+1atWSJG3dulXnzp3LVcKJ/OvMmdN6a+gQnTx5Qt4+PipfvoJmfPypIurWkyQNHvKmXEwuGvjaq7qa/v8bkr81ws5RA7iZ3/fssdpQfPKEdyVJrZ5oq5GjYxTVraeuXL6ssaNG6MKFFFWvUVPTPvw422zTdwu/VdXqNVS6TNm7Gj/yJx6RaIxD7CM5ZMgQnTlzRjNnzlSBAgUkXdvi5eWXX5avr6/ee++9XI3HPpJA/sU+kkD+Zc99JDcl2G67wdrl/O7c6R7lEIlksWLFtG7dOlWoUMGqff/+/apbt65Onz6dq/FIJIH8i0QSyL/smUhuPmS7RPLhsvk3kXSIm20yMjK0b9++bO379u1TVhZ/aQAAANviXhtjHGL7n27duqlHjx5KSEjQww8/LEnatGmTxo0bp27dutk5OgAAANyMQySSEyZMUFBQkCZOnKikpCRJUnBwsAYPHqyBAwfaOToAAJDv5ffSoY04xBrJf0pJSZGkbI+7yw3WSAL5F2skgfzLnmsktyTabo3kQ2Xy7xpJh6hI/tO/SSABAACMYPsfYxwikSxTpoxMplv/Bh46dOguRgMAAICccIhE8rXXXrN6n56eru3bt+unn37S4MGD7RMUAABwGrepZ+E2HCKR7Nev303bP/jgA/33v/+9y9EAAAAgJxxiH8lbad68ub799lt7hwEAAPI59pE0xiEqkrfyn//8RwEBAfYOAwAA5Hf5PeOzEYdIJGvUqGF1s43ZbFZycrJOnjypDz/80I6RAQAA4FYcIpFs27at1XsXFxcVK1ZMjRo10gMPPGCfoAAAgNNg+x9jHG5D8rzAhuRA/sWG5ED+Zc8Nybf/ecFmY9cI9bHZ2PbmEBXJf7py5YquXr1q1cYm5QAAwJbY/scYh7hr++LFi+rbt68CAwPl5eWlwoULW70AAADgeBwikXz99de1cuVKzZgxQ25ubpo1a5aio6MVEhKizz//3N7hAQCAfI7tf4xxiDWSpUqV0ueff65GjRrJ19dX27ZtU1hYmObMmaMvv/xSS5cuzdV4rJEE8i/WSAL5lz3XSO44Yrs1ktVK5d81kg5RkTxz5ozKli0r6dp6yDNnzkiSHnnkEa1du9aeoQEAAGdASdIQh0gky5Ytq8TEREnSAw88oPnz50uSfvjhB/n7+9sxMgAA4AxMNvwvP3OIRLJbt27asWOHJOmNN97QBx98IHd3d/Xv31+DBw+2c3QAAAC4GYdYI3mjP//8U1u3blVYWJiqVq2a6/NZIwnkX6yRBPIve66R3HU01WZjVynhbbOx7c3uFcn09HQ99thjOnDggKUtNDRU7dq1M5REAgAA4O6w+4bkhQoV0s6dO+0dBgAAcGL5eyWj7di9IilJnTt31qeffmrvMAAAAJALdq9ISlJGRoY+++wzLV++XA8++KC8vLysjk+aNMlOkQEAAKdASdIQuyaShw4dUunSpbV7927VrFlTkvTHH39Y9THx8EsAAACHZNe7tgsUKKCkpCQFBgZKkp599llNmzZNxYsX/1fjctc2kH9x1zaQf9nzru09f1+02diV7vO6c6d7lF3XSN6Yw/7444+6eNF2v5EAAADIOw6xRvI6B9zSEgAAOAFW0hlj14qkyWTKtgaSNZEAAOBuc5RHbY8cOdKSH11/PfDAA5bjV65cUZ8+fVSkSBF5e3urffv2On78uNUYR44cUcuWLeXp6anAwEANHjxYGRm2Wfdn14qk2WxW165d5ebmJunal9OrV69sd20vWLDAHuEBAADcdZUqVdLy5cst7wsW/F+61r9/fy1ZskTffPON/Pz81LdvX7Vr107r16+XJGVmZqply5YKCgrSb7/9pqSkJHXp0kWFChXS2LFj8zxWuyaSUVFRVu87d+5sp0gAAIBTc6AJ0YIFCyooKChb+/nz5/Xpp59q3rx5evTRRyVJs2fPVnh4uDZu3Kg6derol19+0e+//67ly5erePHiql69ukaPHq0hQ4Zo5MiRcnV1zdtY83S0XJo9e7Y9Lw8AAGBzaWlpSktLs2pzc3OzzMje6MCBAwoJCZG7u7siIiIUExOjUqVKaevWrUpPT1eTJk0sfR944AGVKlVKGzZsUJ06dbRhwwZVqVLFagecyMhI9e7dW3v27FGNGjXy9LM5xJNtAAAA7Mlkw/9iYmLk5+dn9YqJiblpHLVr11ZsbKx++uknzZgxQ4mJiapfv74uXLig5ORkubq6yt/f3+qc4sWLKzk5WZKUnJycbRvF6++v98lLDnXXNgAAQH4zdOhQDRgwwKrtVtXI5s2bW35dtWpV1a5dW6GhoZo/f748PDxsGqcRVCQBAIDTM5ls93Jzc5Ovr6/V61aJ5I38/f1Vvnx5HTx4UEFBQbp69arOnTtn1ef48eOWNZVBQUHZ7uK+/v5m6y7/LRJJAAAAB5WamqqEhAQFBwfrwQcfVKFChbRixQrL8f379+vIkSOKiIiQJEVERGjXrl06ceKEpc+yZcvk6+urihUr5nl8TG0DAACn5yg3bQ8aNEitW7dWaGiojh07phEjRqhAgQJ67rnn5Ofnpx49emjAgAEKCAiQr6+vXnnlFUVERKhOnTqSpKZNm6pixYp6/vnnNX78eCUnJ+utt95Snz59clwFzQ0SSQAAAAfJJI8eParnnntOp0+fVrFixfTII49o48aNKlasmCRp8uTJcnFxUfv27ZWWlqbIyEh9+OGHlvMLFCigxYsXq3fv3oqIiJCXl5eioqI0atQom8RrMufD5xJesc3m7QAcQHpGlr1DAGAjPu72W3H3x/FLNhu7fHFPm41tb1QkAQCA0zM5SknyHsPNNgAAADCEiiQAAHB6JgqShlCRBAAAgCFUJAEAgNOjIGkMFUkAAAAYQkUSAACAkqQhJJIAAMDpsf2PMUxtAwAAwBAqkgAAwOmx/Y8xVCQBAABgCBVJAADg9ChIGkNFEgAAAIZQkQQAAKAkaQgVSQAAABhCRRIAADg99pE0hkQSAAA4Pbb/MYapbQAAABhCRRIAADg9CpLGUJEEAACAIVQkAQCA02ONpDFUJAEAAGAIFUkAAABWSRpCRRIAAACGUJEEAABOjzWSxpBIAgAAp0ceaQxT2wAAADCEiiQAAHB6TG0bQ0USAAAAhlCRBAAATs/EKklDqEgCAADAECqSAAAAFCQNoSIJAAAAQ6hIAgAAp0dB0hgSSQAA4PTY/scYprYBAABgCBVJAADg9Nj+xxgqkgAAADCEiiQAAAAFSUOoSAIAAMAQKpIAAMDpUZA0hookAAAADKEiCQAAnB77SBpDIgkAAJwe2/8Yw9Q2AAAADKEiCQAAnB5T28ZQkQQAAIAhJJIAAAAwhEQSAAAAhrBGEgAAOD3WSBpDRRIAAACGUJEEAABOj30kjSGRBAAATo+pbWOY2gYAAIAhVCQBAIDToyBpDBVJAAAAGEJFEgAAgJKkIVQkAQAAYAgVSQAA4PTY/scYKpIAAAAwhIokAABweuwjaQwVSQAAABhCRRIAADg9CpLGkEgCAACQSRrC1DYAAAAMoSIJAACcHtv/GENFEgAAAIZQkQQAAE6P7X+MoSIJAAAAQ0xms9ls7yAAo9LS0hQTE6OhQ4fKzc3N3uEAyEP8fAOOj0QS97SUlBT5+fnp/Pnz8vX1tXc4APIQP9+A42NqGwAAAIaQSAIAAMAQEkkAAAAYQiKJe5qbm5tGjBjBQnwgH+LnG3B83GwDAAAAQ6hIAgAAwBASSQAAABhCIgkAAABDSCThtA4fPiyTyaT4+PgcnzNy5EhVr17dZjEBuKZr165q27Ztrs4xmUxatGiRTeIBcHMkkk6oa9euMplMGjdunFX7okWLZLLxU+uvJ283vjp37mzT6zqy2NhY+fv72zsMQNL//ny48XXw4EF7h2YXpUuX1pQpU+wdBuCwCto7ANiHu7u73n33Xb300ksqXLjwXb/+8uXLValSJct7Dw+PbH3MZrMyMzNVsCD/mwJ3U7NmzTR79myrtmLFilm9v3r1qlxdXe9mWAAcEBVJJ9WkSRMFBQUpJibmln2+/fZbVapUSW5ubipdurQmTpxodbx06dIaO3asunfvLh8fH5UqVUoff/xxjq5fpEgRBQUFWV5+fn5avXq1TCaTfvzxRz344INyc3PTunXrlJCQoDZt2qh48eLy9vbWQw89pOXLl1uNd7MpLX9/f8XGxlreb968WTVq1JC7u7tq1aql7du3W/W/WWUwJ1XaWbNmKTw8XO7u7nrggQf04YcfWo5dr8AuWLBAjRs3lqenp6pVq6YNGzZIklavXq1u3brp/PnzlsrPyJEjc/QdArbi5uZm9fMZFBSkxx57TH379tVrr72mokWLKjIyUpI0adIkValSRV5eXipZsqRefvllpaamWsa62XKQKVOmqHTp0pb3mZmZGjBggPz9/VWkSBG9/vrrunFnuptVBqtXr37bn5e//vpLzzzzjPz9/RUQEKA2bdro8OHDluPXp88nTJig4OBgFSlSRH369FF6erokqVGjRvrzzz/Vv39/y88nAGskkk6qQIECGjt2rKZPn66jR49mO75161Y988wz6tChg3bt2qWRI0dq+PDhVomZJE2cONGSlL388svq3bu39u/f/69ie+ONNzRu3Djt3btXVatWVWpqqlq0aKEVK1Zo+/btatasmVq3bq0jR47keMzU1FS1atVKFStW1NatWzVy5EgNGjToX8UpSXPnztXbb7+td955R3v37tXYsWM1fPhwxcXFWfUbNmyYBg0apPj4eJUvX17PPfecMjIyVLduXU2ZMkW+vr5KSkpSUlJSnsQF2EJcXJxcXV21fv16zZw5U5Lk4uKiadOmac+ePYqLi9PKlSv1+uuv52rciRMnKjY2Vp999pnWrVunM2fOaOHChf8q1vT0dEVGRsrHx0e//vqr1q9fL29vbzVr1kxXr1619Fu1apUSEhK0atUqxcXFKTY21vLn3IIFC1SiRAmNGjXK8vMJ4AZmOJ2oqChzmzZtzGaz2VynTh1z9+7dzWaz2bxw4ULz9f8lOnbsaH788cetzhs8eLC5YsWKlvehoaHmzp07W95nZWWZAwMDzTNmzLjltRMTE82SzB4eHmYvLy/La9u2beZVq1aZJZkXLVp0x89QqVIl8/Tp0y3vJZkXLlxo1cfPz888e/Zss9lsNn/00UfmIkWKmC9fvmw5PmPGDLMk8/bt281ms9k8e/Zss5+fn9UY//xOzGazecSIEeZq1apZ3pcrV848b948q3NGjx5tjoiIsPq8s2bNshzfs2ePWZJ57969t7wuYC9RUVHmAgUKWP18PvXUU+aGDRuaa9Soccfzv/nmG3ORIkUs72/8mTGbzebJkyebQ0NDLe+Dg4PN48ePt7xPT083lyhRwvLnlNl87c+byZMnW41TrVo184gRIyzv//nnwJw5c8wVKlQwZ2VlWY6npaWZPTw8zD///LPls4aGhpozMjIsfZ5++mnzs88+e9vrAvgfKpJO7t1331VcXJz27t1r1b53717Vq1fPqq1evXo6cOCAMjMzLW1Vq1a1/NpkMikoKEgnTpyQJDVv3lze3t7y9va2Wg8pSV9//bXi4+Mtr4oVK1qO1apVy6pvamqqBg0apPDwcPn7+8vb21t79+7NVUXyenXT3d3d0hYREZHj82/m4sWLSkhIUI8ePSyf09vbW2PGjFFCQoJV339+T8HBwZJk+Z4AR9O4cWOrn89p06ZJkh588MFsfZcvX67HHntM9913n3x8fPT888/r9OnTunTpUo6udf78eSUlJal27dqWtoIFC2b7cyC3duzYoYMHD8rHx8fysxkQEKArV65Y/XxWqlRJBQoUsLwPDg7mZxPIBe5icHINGjRQZGSkhg4dqq5du+b6/EKFClm9N5lMysrKknRt7eDly5dv2q9kyZIKCwu76ZheXl5W7wcNGqRly5ZpwoQJCgsLk4eHh5566imr6SmTyZRtTdX1dU455eLikqsxrq8D++STT6z+EpRk9ReTZP35r6+zuv49AY7Gy8vrpj+fN/5sHj58WK1atVLv3r31zjvvKCAgQOvWrVOPHj109epVeXp65vrn6laM/Hw++OCDmjt3brZj/7xx6HZ/hgG4MxJJaNy4capevboqVKhgaQsPD9f69eut+q1fv17ly5fPliTdyn333Zcn8a1fv15du3bVk08+KenaXxD/XDAvXfuL4Z/rlw4cOGBVEQkPD9ecOXN05coVS1Vy48aN2ca4cOGCLl68aPkL83Z7TBYvXlwhISE6dOiQOnXqZPjzubq6WlV5gXvF1q1blZWVpYkTJ8rF5doE1/z58636FCtWTMnJyTKbzZZ/RP3z58rPz0/BwcHatGmTGjRoIEnKyMjQ1q1bVbNmTatx/vkznpKSosTExFvGVrNmTX399dcKDAyUr6+v4c/Izydwe0xtQ1WqVFGnTp0s01eSNHDgQK1YsUKjR4/WH3/8obi4OL3//vt2uRHk/vvv14IFCxQfH68dO3aoY8eO2SoGjz76qN5//31t375d//3vf9WrVy+rSkPHjh1lMpn0wgsv6Pfff9fSpUs1YcIEqzFq164tT09Pvfnmm0pISNC8efOy3Vx0o+joaMXExGjatGn6448/tGvXLs2ePVuTJk3K8ecrXbq0UlNTtWLFCp06dSrHU4KAvYWFhSk9PV3Tp0/XoUOHNGfOHMtNONc1atRIJ0+e1Pjx45WQkKAPPvhAP/74o1Wffv36ady4cVq0aJH27dunl19+WefOnbPq8+ijj2rOnDn69ddftWvXLkVFRd32H7WdOnVS0aJF1aZNG/36669KTEzU6tWr9eqrr970BsNbKV26tNauXau///5bp06dyvF5gLMgkYQkadSoUVbJWc2aNTV//nx99dVXqly5st5++22NGjXK0PT3vzVp0iQVLlxYdevWVevWrRUZGWlVqZCu3fVZsmRJ1a9fXx07dtSgQYPk6elpOe7t7a0ffvhBu3btUo0aNTRs2DC9++67VmMEBAToiy++0NKlS1WlShV9+eWXd9yKp2fPnpo1a5Zmz56tKlWqqGHDhoqNjVWZMmVy/Pnq1q2rXr166dlnn1WxYsU0fvz4HJ8L2FO1atU0adIkvfvuu6pcubLmzp2bbUux8PBwffjhh/rggw9UrVo1bd68Ods/SAcOHKjnn39eUVFRioiIkI+Pj2UG4rqhQ4eqYcOGatWqlVq2bKm2bduqXLlyt4zN09NTa9euValSpdSuXTuFh4erR48eunLlSq4qlKNGjdLhw4dVrly5bHtpApBM5hsXnQAAAAA5QEUSAAAAhpBIAgAAwBASSQAAABhCIgkAAABDSCQBAABgCIkkAAAADCGRBAAAgCEkkgAAADCERBKAw+ratavatm1red+oUSO99tprdz2O1atXy2QyZXtsHwA4OxJJALnWtWtXmUwmmUwmubq6KiwsTKNGjVJGRoZNr7tgwQKNHj06R31J/gDA9graOwAA96ZmzZpp9uzZSktL09KlS9WnTx8VKlRIQ4cOtep39epVubq65sk1AwIC8mQcAEDeoCIJwBA3NzcFBQUpNDRUvXv3VpMmTfT9999bpqPfeecdhYSEqEKFCpKkv/76S88884z8/f0VEBCgNm3a6PDhw5bxMjMzNWDAAPn7+6tIkSJ6/fXXZTabra5549R2WlqahgwZopIlS8rNzU1hYWH69NNPdfjwYTVu3FiSVLhwYZlMJnXt2lWSlJWVpZiYGJUpU0YeHh6qVq2a/vOf/1hdZ+nSpSpfvrw8PDzUuHFjqzgBAP9DIgkgT3h4eOjq1auSpBUrVmj//v1atmyZFi9erPT0dEVGRsrHx0e//vqr1q9fL29vbzVr1sxyzsSJExUbG6vPPvtM69at05kzZ7Rw4cLbXrNLly768ssvNW3aNO3du1cfffSRvL29VbJkSX377beSpP379yspKUlTp06VJMXExOjzzz/XzJkztWfPHvXv31+dO3fWmjVrJF1LeNu1a6fWrVsrPj5ePXv21BtvvGGrrw0A7mlMbQP4V8xms1asWKGff/5Zr7zyik6ePCkvLy/NmjXLMqX9xRdfKCsrS7NmzZLJZJIkzZ49W/7+/lq9erWaNm2qKVOmaOjQoWrXrp0kaebMmfr5559ved0//vhD8+fP17Jly9SkSRNJUtmyZS3Hr0+DBwYGyt/fX9K1CubYsWO1fPlyRUREWM5Zt26dPvroIzVs2FAzZsxQuXLlNHHiRElShQoVtGvXLr377rt5+K0BQP5AIgnAkMWLF8vb21vp6enKyspSx44dNXLkSPXp00dVqlSxWhe5Y8cOHTx4UD4+PlZjXLlyRQkJCTp//rySkpJUu3Zty7GCBQuqVq1a2aa3r4uPj1eBAgXUsGHDHMd88OBBXbp0SY8//rhV+9WrV1WjRg1J0t69e63ikGRJOgEA1kgkARjSuHFjzZgxQ66urgoJCVHBgv/748TLy8uqb2pqqh588EHNnTs32zjFihUzdH0PD49cn5OamipJWrJkie677z6rY25ubobiAABnRiIJwBAvLy+FhYXlqG/NmjX19ddfKzAwUL6+vjftExwcrE2bNqlBgwaSpIyMDG3dulU1a9a8af8qVaooKytLa9assUxt/9P1imhmZqalrWLFinJzc9ORI0duWckMDw/X999/b9W2cePGO39IAHBC3GwDwOY6deqkokWLqk2bNvr111+VmJio1atX69VXX9XRo0clSf369dO4ceO0aNEi7du3Ty+//PJt94AsXbq0oqKi1L17dy1atMgy5vz58yVJoaGhMplMWrx4sU6ePKnU1FT5+Pho0KBB6t+/v+Li4pSQkKBt27Zp+vTpiouLkyT16tVLBw4c0ODBg7V//37NmzdPsbGxtv6KAOCeRCIJwOY8PT21du1alSpVSu3atVN4eLh69OihK1euWCqUAwcO1PPPP6+oqChFRETIx8dHTz755G3HnTFjhp566im9/PLLeuCBB/TCCy/o4sWLkqT77rtP0dHReuONN1S8eHH17dtXkjR69GgNHz5cMTExCg8PV7NmzbRkyRKVKVNGklSqVCl9++23WrRokapVq6aZM2dq7NixNvx2AODeZTLfaiU7AAAAcBtUJAEAAGAIiSQAAAAMIZEEAACAISSSAAAAMIREEgAAAIaQSAIAAMAQEkkAAAAYQiIJAAAAQ0gkAQAAYAiJJAAAAAwhkQQAAIAh/wcSi1GiUD0iZgAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = trainer.predict(test_dataset)\n",
        "\n",
        "# Get predicted labels\n",
        "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
        "\n",
        "# Evaluate with per-class precision, recall, F1-score\n",
        "print(classification_report(y_test, predicted_labels))\n",
        "\n",
        "# Compute and plot the confusion matrix\n",
        "cm = confusion_matrix(y_test, predicted_labels)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Non-Fraudulent\", \"Fraudulent\"],\n",
        "            yticklabels=[\"Non-Fraudulent\", \"Fraudulent\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNyytsLXn2vA"
      },
      "source": [
        "## Save model to Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "8dkVTLviKdX5",
        "outputId": "080c7685-b6d7-4eb6-b31f-3f4ec50f42e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/pcloud/job_catcher-bert-base-uncased/commit/359edfd736108f7494a260a899009ed5e5d60bb4', commit_message='Upload tokenizer', commit_description='', oid='359edfd736108f7494a260a899009ed5e5d60bb4', pr_url=None, repo_url=RepoUrl('https://huggingface.co/pcloud/job_catcher-bert-base-uncased', endpoint='https://huggingface.co', repo_type='model', repo_id='pcloud/job_catcher-bert-base-uncased'), pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "trainer.save_model(\"./fine_tuned_bert\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_bert\")\n",
        "\n",
        "repo = \"pcloud/job_catcher-bert-base-uncased\"\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"./fine_tuned_bert\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_bert\")\n",
        "\n",
        "model.push_to_hub(repo)\n",
        "tokenizer.push_to_hub(repo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gqjm9yQlmsub",
        "outputId": "6550087d-7f55-43cc-d6e1-b88ca7c31529"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Fake Job Posting'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Replace with your uploaded model name\n",
        "model_name = repo\n",
        "\n",
        "# Load model and tokenizer from Hugging Face Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "def predict(job_text, model, tokenizer):\n",
        "    inputs = tokenizer(\n",
        "        job_text,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    prediction = torch.argmax(logits, dim=-1).item()  # 0 = Real, 1 = Fake\n",
        "    return \"Fake Job Posting\" if prediction == 1 else \"Real Job Posting\"\n",
        "\n",
        "# Example usage:\n",
        "new_job_posting = \"Alliance is looking for people who are passionate about both the work they do and the life they live.We are a high energy, goal oriented organization who is seeking out like minded professionals to assist our organization in continued growth. Alliance is an industry leader in providing innovative point-of-sale (POS) payment acceptance capabilities -- and backing them up with service, financial strength and stability.We offer flexibility, uncapped success, autonomy and life/work balance.You would be hard pressed to find a company that will treat you better. We offer daily/monthly contests and yearly bonuses for top employees. Come see why AMS is one of the fastest growing companies.\"\n",
        "result = predict(new_job_posting, model, tokenizer)\n",
        "result\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}