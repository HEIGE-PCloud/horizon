{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def load_data(path: Path) -> pd.DataFrame:\n",
    "    data = pd.read_csv(path)\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            \"job_description\": data[\n",
    "                [\"company_profile\", \"description\", \"requirements\", \"benefits\"]\n",
    "            ]\n",
    "            .fillna(\"\")\n",
    "            .agg(\" \".join, axis=1),\n",
    "            \"fraudulent\": data[\"fraudulent\"],\n",
    "        }\n",
    "    )\n",
    "    data = data.drop_duplicates(subset=[\"job_description\"], keep=\"first\")\n",
    "    return data\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def split_data(data: pd.DataFrame):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data[\"job_description\"],\n",
    "        data[\"fraudulent\"],\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=data[\"fraudulent\"],\n",
    "    )\n",
    "\n",
    "    # Convert to DataFrame for easy manipulation\n",
    "    train_df = pd.DataFrame({'job_description': X_train, 'fraudulent': y_train})\n",
    "\n",
    "    # Separate fraudulent (y=1) and non-fraudulent (y=0) samples\n",
    "    fraudulent_df = train_df[train_df['fraudulent'] == 1]\n",
    "    non_fraudulent_df = train_df[train_df['fraudulent'] == 0]\n",
    "    print(fraudulent_df.shape, non_fraudulent_df.shape)\n",
    "\n",
    "    # Oversample each fraudulent job description exactly 21 times\n",
    "    fraudulent_df_oversampled = pd.concat([fraudulent_df] * 21, ignore_index=True)\n",
    "\n",
    "    # Combine the oversampled fraudulent data with the original non-fraudulent data\n",
    "    train_df_oversampled = pd.concat([non_fraudulent_df, fraudulent_df_oversampled], ignore_index=True)\n",
    "\n",
    "    # Shuffle the data\n",
    "    train_df_oversampled = train_df_oversampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Extract the oversampled X_train and y_train\n",
    "    X_train: pd.Series[str] = train_df_oversampled['job_description']\n",
    "    y_train: pd.Series[int] = train_df_oversampled['fraudulent']\n",
    "\n",
    "    # apply clean_text function to the training and test data\n",
    "\n",
    "    X_train = X_train.apply(clean_text)\n",
    "    X_test = X_test.apply(clean_text)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "from pathlib import Path\n",
    "\n",
    "path = kagglehub.dataset_download(\"shivamb/real-or-fake-fake-jobposting-prediction\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "data = load_data(Path(path + \"/fake_job_postings.csv\"))\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(data)\n",
    "\n",
    "print(X_train.head())\n",
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load BERT-large tokenizer\n",
    "model_name = \"bert-large-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def preprocess_data(texts, labels):\n",
    "    return tokenizer(texts, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# Tokenize training & test sets\n",
    "train_encodings = preprocess_data(X_train.tolist(), y_train.tolist())\n",
    "test_encodings = preprocess_data(X_test.tolist(), y_test.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class FakeJobDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = FakeJobDataset(train_encodings, y_train.tolist())\n",
    "test_dataset = FakeJobDataset(test_encodings, y_test.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load BERT-large with classification head\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",      # Directory for model checkpoints\n",
    "    evaluation_strategy=\"epoch\", # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",       # Save model at each epoch\n",
    "    per_device_train_batch_size=4,  # Reduce batch size due to memory limits\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,          # Adjust based on dataset size\n",
    "    learning_rate=2e-5,          # Recommended LR for fine-tuning BERT\n",
    "    weight_decay=0.01,           # Regularization\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    "    save_total_limit=2,          # Keep only the 2 latest models\n",
    "    fp16=True,                   # Enable mixed precision (A100 supports it)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model locally\n",
    "model.save_pretrained(\"bert-large-fake-job-classifier\")\n",
    "tokenizer.save_pretrained(\"bert-large-fake-job-classifier\")\n",
    "\n",
    "# Upload to Hugging Face Hub\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()  # Logs into Hugging Face\n",
    "\n",
    "model.push_to_hub(\"pcloud/job_catcher-bert-large-uncased\")\n",
    "tokenizer.push_to_hub(\"pcloud/job_catcher-bert-large-uncased\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
